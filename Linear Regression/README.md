# ğŸ“ˆ Linear Regression Cross-Validation Playground

<div align="center">

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-v1.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

<i>Explore, compare, and visualize linear regression accuracy using four powerful cross-validation techniques!</i>

</div>

---

## ğŸš€ Overview

This project demonstrates how to evaluate a linear regression model using **Holdout**, **LOOCV**, **Stratified K-Fold**, and **K-Fold** cross-validation strategies in Python. Results are compared using the RÂ² score for each method.

> ğŸ¯ **Goal:** See how different validation strategies impact model accuracy and reliability.

---

## ğŸ“ Project Structure

```
Linear Regression/
â”œâ”€â”€ linear_regression_cv.py.py   # Main script
â”œâ”€â”€ sal.csv                     # Dataset file
â”œâ”€â”€ README.md                   # This documentation
```

---

## ğŸ› ï¸ Technologies Used

| Python | Pandas | NumPy | scikit-learn |
| :----: | :----: | :---: | :----------: |
|   ğŸ   |   ğŸ¼   |  ğŸ”¢   |      ğŸ¤–      |

---

## ğŸ“‹ Prerequisites

```powershell
pip install pandas numpy scikit-learn
```

---

## ğŸ“Š Data & Features

- **Input:** `sal.csv` (all columns except last are features)
- **Target:** Last column in `sal.csv`

---

## ğŸ§‘â€ğŸ’» How It Works

### 1ï¸âƒ£ Data Loading & Preparation

```python
df = pd.read_csv("sal.csv")
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
```

### 2ï¸âƒ£ Model Setup

```python
model = LinearRegression()
```

### 3ï¸âƒ£ Validation Techniques

| Method               | What It Does                       | When To Use              |
| -------------------- | ---------------------------------- | ------------------------ |
| ğŸŸ¦ Holdout           | Train/test split (80/20)           | Quick baseline           |
| ğŸŸ© LOOCV             | Each sample is tested once         | Small datasets, unbiased |
| ğŸŸ¨ Stratified K-Fold | Folds preserve target distribution | Imbalanced targets       |
| ğŸŸ§ K-Fold            | Random folds, average results      | General purpose          |

---

## ğŸ”¬ Workflow Breakdown

<details>
<summary>ğŸ” <b>Step-by-step process</b></summary>

1. **Holdout Validation**
   - Split data, train, test, compute RÂ²
2. **LOOCV**
   - For each sample: train on all others, test on one
3. **Stratified K-Fold**
   - Bin target, split into stratified folds, train/test
4. **K-Fold**
   - Shuffle, split into 5 folds, train/test
5. **Results**
   - Print RÂ² for each method

</details>

---

## ğŸ“ˆ Output Example

```
Model Accuracy (RÂ² Score):
(i) Holdout Validation               : 0.9123
(ii) Leave-One-Out CV (LOOCV)        : 0.9056
(iii) Stratified K-Fold CV           : 0.9102
(iv) K-Fold CV                       : 0.9087
```

---

## ğŸ§  Key Insights

- **Holdout**: Fast, but can be sensitive to random splits
- **LOOCV**: Most robust for small datasets
- **Stratified K-Fold**: Best for imbalanced targets
- **K-Fold**: Reliable for general use

---

## ğŸ“ Customization

- Change `n_bins` or `n_splits` in the script to adjust folds/bins
- Replace `sal.csv` with your own dataset (same format)

---

## ğŸ“š References

- [scikit-learn documentation](https://scikit-learn.org/stable/documentation.html)
- [Cross-validation strategies](https://scikit-learn.org/stable/modules/cross_validation.html)

---

<div align="center">

![Data Analysis](https://img.shields.io/badge/Data%20Analysis-Complete-brightgreen?style=for-the-badge&logo=chart-dot-js)
![Validation Methods](https://img.shields.io/badge/Validation-4%20Types-blue?style=for-the-badge&logo=check)
![Code Quality](https://img.shields.io/badge/Code%20Quality-High-yellow?style=for-the-badge&logo=code)

<i>Made with â¤ï¸ for the Machine Learning Community</i>

</div>
